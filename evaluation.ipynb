{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9955283,"sourceType":"datasetVersion","datasetId":6122741}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#1\nfile_path = '/kaggle/input/gyafcdataee/Entertainment_Music/train/informal'   # Replace 'your_text_file.txt' with the path to your text file\nwith open(file_path, 'r') as file:\n    X = file.readlines()\n\nfor i  in range(len(X)):\n  X[i] = X[i].split(\"\\n\")[0]\n  X[i] = X[i].lower()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T04:54:38.971738Z","iopub.execute_input":"2024-11-20T04:54:38.972115Z","iopub.status.idle":"2024-11-20T04:54:39.044562Z","shell.execute_reply.started":"2024-11-20T04:54:38.972082Z","shell.execute_reply":"2024-11-20T04:54:39.043606Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#Adding token 0 to informal sentences\ny  = []\nfor i in range(len(X)):\n    y.append(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T04:54:40.924697Z","iopub.execute_input":"2024-11-20T04:54:40.925049Z","iopub.status.idle":"2024-11-20T04:54:40.936927Z","shell.execute_reply.started":"2024-11-20T04:54:40.925017Z","shell.execute_reply":"2024-11-20T04:54:40.935678Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"file_path = '/kaggle/input/gyafcdataee/Entertainment_Music/train/formal'   # Replace 'your_text_file.txt' with the path to your text file\nwith open(file_path, 'r') as file:\n    X_formal = file.readlines()\n\nfor i  in range(len(X_formal)):\n  X_formal[i] = X_formal[i].split(\"\\n\")[0]\n  X_formal[i] = X_formal[i].lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T04:54:43.091869Z","iopub.execute_input":"2024-11-20T04:54:43.092249Z","iopub.status.idle":"2024-11-20T04:54:43.163154Z","shell.execute_reply.started":"2024-11-20T04:54:43.092216Z","shell.execute_reply":"2024-11-20T04:54:43.162360Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"#Adding token 1 to formal sentences\ny_formal = []\nfor i in range(len(X_formal)):\n  y_formal.append(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T04:54:45.347874Z","iopub.execute_input":"2024-11-20T04:54:45.348242Z","iopub.status.idle":"2024-11-20T04:54:45.358858Z","shell.execute_reply.started":"2024-11-20T04:54:45.348209Z","shell.execute_reply":"2024-11-20T04:54:45.357469Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"dict_formal = {\"X\" : X_formal,\n               \"y\": y_formal}\ndict_informal = {\"X\": X,\n                 \"y\": y}\nimport pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\ndf_formal = pd.DataFrame(dict_formal)\ndf_informal = pd.DataFrame(dict_informal)\ndf = pd.concat([df_formal,df_informal],axis = 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T04:54:47.698370Z","iopub.execute_input":"2024-11-20T04:54:47.698792Z","iopub.status.idle":"2024-11-20T04:54:48.088376Z","shell.execute_reply.started":"2024-11-20T04:54:47.698759Z","shell.execute_reply":"2024-11-20T04:54:48.087435Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Step 2: Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['X'].tolist(), df['y'].tolist(), test_size=0.2, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T04:54:51.945175Z","iopub.execute_input":"2024-11-20T04:54:51.945726Z","iopub.status.idle":"2024-11-20T04:54:52.005236Z","shell.execute_reply.started":"2024-11-20T04:54:51.945691Z","shell.execute_reply":"2024-11-20T04:54:52.004164Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# Step 2: Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['X'].tolist(), df['y'].tolist(), test_size=0.2, random_state=42)\n\n# Step 3: Tokenization\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\ntokenized_inputs_train = tokenizer(X_train, padding=True, truncation=True, return_tensors=\"pt\")\ntokenized_inputs_test = tokenizer(X_test, padding=True, truncation=True, return_tensors=\"pt\")\n\n# Step 4: Create PyTorch Dataset\nclass FormalityDataset(Dataset):\n    def __init__(self, tokenized_inputs, labels):\n        self.tokenized_inputs = tokenized_inputs\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.tokenized_inputs[\"input_ids\"][idx],\n            \"attention_mask\": self.tokenized_inputs[\"attention_mask\"][idx],\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\ntrain_dataset = FormalityDataset(tokenized_inputs_train, y_train)\ntest_dataset = FormalityDataset(tokenized_inputs_test, y_test)\n\n# Step 5: Fine-Tune the Model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=8)\n\n# Move the model to the appropriate device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Move input tensors to the same device as the model\n# inputs = {key: value.to(device) for key, value in batch.items()}\n# Fine-tuning loop\nmodel.train()\nfor epoch in range(3):  # Adjust number of epochs as needed\n    for batch in train_dataloader:\n        optimizer.zero_grad()\n\n        inputs = {key: value.to(device) for key, value in batch.items()}\n        outputs = model(**inputs)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n# Step 6: Save Model and Tokenizer Configurations\nmodel.save_pretrained(\"model_configs_fscore\")\ntokenizer.save_pretrained(\"tokenizer_configs_fscore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\n# Ensure the destination directory exists\nkaggle_config_dir = os.path.expanduser('~/.config/kaggle/')\nos.makedirs(kaggle_config_dir, exist_ok=True)\n\n# Move the kaggle.json file\nshutil.copy('/kaggle/input/kuggle/kaggle.json', os.path.join(kaggle_config_dir, 'kaggle.json'))\n\nprint(\"kaggle.json file moved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T05:51:32.346789Z","iopub.execute_input":"2024-11-20T05:51:32.347179Z","iopub.status.idle":"2024-11-20T05:51:32.356438Z","shell.execute_reply.started":"2024-11-20T05:51:32.347148Z","shell.execute_reply":"2024-11-20T05:51:32.355477Z"}},"outputs":[{"name":"stdout","text":"kaggle.json file moved successfully!\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"!kaggle kernels output subram/evaluation -p /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T04:52:46.920099Z","iopub.execute_input":"2024-11-20T04:52:46.921073Z","iopub.status.idle":"2024-11-20T04:52:53.597678Z","shell.execute_reply.started":"2024-11-20T04:52:46.921035Z","shell.execute_reply":"2024-11-20T04:52:53.596465Z"}},"outputs":[{"name":"stdout","text":"Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.config/kaggle/kaggle.json'\nOutput file downloaded to /kaggle/working/model_configs_fscore/config.json\nOutput file downloaded to /kaggle/working/model_configs_fscore/model.safetensors\nOutput file downloaded to /kaggle/working/tokenizer_configs_fscore/special_tokens_map.json\nOutput file downloaded to /kaggle/working/tokenizer_configs_fscore/tokenizer_config.json\nOutput file downloaded to /kaggle/working/tokenizer_configs_fscore/vocab.txt\nKernel log downloaded to /kaggle/working/evaluation.log \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Step 1: Load the Model and Tokenizer\nmodel = BertForSequenceClassification.from_pretrained(\"/kaggle/working/model_configs_fscore\")\ntokenizer = BertTokenizer.from_pretrained(\"/kaggle/working/tokenizer_configs_fscore\")\n\n# Step 2: Tokenization\ntext = input()\ninputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n\n# Step 3: Prepare Input Tensors\ninput_ids = inputs[\"input_ids\"]\nattention_mask = inputs[\"attention_mask\"]\n\n# Move tensors to the appropriate device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninput_ids = input_ids.to(device)\nattention_mask = attention_mask.to(device)\nmodel.to(device)\n\n# Step 4: Forward Pass\nwith torch.no_grad():\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    logits = outputs.logits\n\n# Apply softmax to obtain probabilities\nprobabilities = torch.softmax(logits, dim=1)\n\n# Extract formality scores\nformality_score = probabilities[0][1].item()  # Probability of class 1 (informal)\n\n\nprint(\"Formality Score:\", formality_score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T04:54:13.951188Z","iopub.execute_input":"2024-11-20T04:54:13.952048Z","iopub.status.idle":"2024-11-20T04:54:25.041116Z","shell.execute_reply.started":"2024-11-20T04:54:13.951990Z","shell.execute_reply":"2024-11-20T04:54:25.040050Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":" Heelooo guys\n"},{"name":"stdout","text":"Formality Score: 0.011326348409056664\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import confusion_matrix\n\n# Step 4: Create PyTorch Dataset\nclass FormalityDataset(Dataset):\n    def __init__(self, tokenized_inputs, labels):\n        self.tokenized_inputs = tokenized_inputs\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.tokenized_inputs[\"input_ids\"][idx],\n            \"attention_mask\": self.tokenized_inputs[\"attention_mask\"][idx],\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\ntokenized_inputs_test = tokenizer(X_test, padding=True, truncation=True, return_tensors=\"pt\")\ntest_dataset = FormalityDataset(tokenized_inputs_test, y_test)\ntest_dataloader = DataLoader(test_dataset, batch_size=8)\n\n\n# List to store true labels and predicted labels\ntrue_labels = []\npredicted_labels = []\n\n# Evaluate the model on the test dataset\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Forward pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=1).tolist()\n\n        # Store true and predicted labels\n        true_labels.extend(labels.tolist())\n        predicted_labels.extend(predictions)\n\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels)\n\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T04:54:56.903781Z","iopub.execute_input":"2024-11-20T04:54:56.904175Z","iopub.status.idle":"2024-11-20T04:58:34.824578Z","shell.execute_reply.started":"2024-11-20T04:54:56.904139Z","shell.execute_reply":"2024-11-20T04:58:34.823452Z"}},"outputs":[{"name":"stdout","text":"Confusion Matrix:\n[[8813 1600]\n [ 988 9637]]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from sklearn.metrics import precision_score , recall_score, f1_score\nprint(\"Recall_score:\",recall_score(true_labels, predicted_labels))\nprint(\"Precision_score:\", precision_score(true_labels,predicted_labels))\nprint(\"F1_score:\", f1_score(true_labels,predicted_labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T04:59:32.745453Z","iopub.execute_input":"2024-11-20T04:59:32.745848Z","iopub.status.idle":"2024-11-20T04:59:32.822489Z","shell.execute_reply.started":"2024-11-20T04:59:32.745814Z","shell.execute_reply":"2024-11-20T04:59:32.821364Z"}},"outputs":[{"name":"stdout","text":"Recall_score: 0.9070117647058824\nPrecision_score: 0.857613241968497\nF1_score: 0.8816210776690148\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\n# Step 1: Load the Universal Sentence Encoder\nprint(\"Loading Universal Sentence Encoder...\")\nuse_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\nprint(\"USE loaded successfully!\")\n\n# Step 2: Define a function to calculate the meaning preservation score\ndef calculate_meaning_preservation(input_sentence, transformed_sentence):\n    \"\"\"\n    Calculate the semantic similarity between input and transformed sentences\n    using the Universal Sentence Encoder (USE).\n    \n    Args:\n    - input_sentence (str): Original sentence before transformation.\n    - transformed_sentence (str): Sentence after transformation.\n    \n    Returns:\n    - float: Meaning preservation score (similarity between -1 and 1).\n    \"\"\"\n    # Create a list of sentences for embedding\n    sentences = [input_sentence, transformed_sentence]\n    \n    # Compute embeddings for both sentences\n    embeddings = use_model(sentences)\n    \n    # Convert embeddings to numpy arrays\n    input_embedding, transformed_embedding = embeddings[0].numpy(), embeddings[1].numpy()\n    \n    # Compute the inner product (cosine similarity) between the two embeddings\n    similarity_score = np.inner(input_embedding, transformed_embedding)\n    \n    return similarity_score\n\n# Step 3: Example usage\n# Define an input sentence and its transformed version\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T05:04:22.468542Z","iopub.execute_input":"2024-11-20T05:04:22.469278Z","iopub.status.idle":"2024-11-20T05:05:08.733589Z","shell.execute_reply.started":"2024-11-20T05:04:22.469238Z","shell.execute_reply":"2024-11-20T05:05:08.732489Z"}},"outputs":[{"name":"stdout","text":"Loading Universal Sentence Encoder...\nUSE loaded successfully!\nMeaning Preservation Score: 0.8413\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"input_sentence = \"The weather is nice today.\"\ntransformed_sentence = \"The weather is pleasant today.\"\n\n# Calculate the meaning preservation score\nscore = calculate_meaning_preservation(input_sentence, transformed_sentence)\nprint(f\"Meaning Preservation Score: {score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T05:07:17.569484Z","iopub.execute_input":"2024-11-20T05:07:17.569836Z","iopub.status.idle":"2024-11-20T05:07:17.611969Z","shell.execute_reply.started":"2024-11-20T05:07:17.569808Z","shell.execute_reply":"2024-11-20T05:07:17.610817Z"}},"outputs":[{"name":"stdout","text":"Meaning Preservation Score: 0.9741\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"final_scores = []\nfor idx, (informal, formal) in enumerate(zip(X, X_formal)):\n    score = calculate_meaning_preservation(informal, formal)\n    final_scores.append(score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T05:12:33.876155Z","iopub.execute_input":"2024-11-20T05:12:33.877071Z","iopub.status.idle":"2024-11-20T05:43:06.903160Z","shell.execute_reply.started":"2024-11-20T05:12:33.877031Z","shell.execute_reply":"2024-11-20T05:43:06.902224Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"np.array(final_scores).mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T05:46:37.088407Z","iopub.execute_input":"2024-11-20T05:46:37.089302Z","iopub.status.idle":"2024-11-20T05:46:37.098351Z","shell.execute_reply.started":"2024-11-20T05:46:37.089260Z","shell.execute_reply":"2024-11-20T05:46:37.097474Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"0.7953845"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"import os\nimport shutil\n\n# Path to your downloaded kaggle.json\nkaggle_json_path = \"/kaggle/input/kuggle/kaggle.json\"  # Replace with the actual path\n\n# Create the .kaggle directory if it doesn't exist\nkaggle_dir = os.path.expanduser(\"~/.kaggle\")\nos.makedirs(kaggle_dir, exist_ok=True)\n\n# Move kaggle.json to the correct directory\nshutil.copy(kaggle_json_path, os.path.join(kaggle_dir, \"kaggle.json\"))\n\n# Set permissions for the file\nos.chmod(os.path.join(kaggle_dir, \"kaggle.json\"), 0o600)\nprint(\"kaggle.json has been successfully moved and configured.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T05:53:02.445025Z","iopub.execute_input":"2024-11-20T05:53:02.446142Z","iopub.status.idle":"2024-11-20T05:53:02.456383Z","shell.execute_reply.started":"2024-11-20T05:53:02.446097Z","shell.execute_reply":"2024-11-20T05:53:02.455280Z"}},"outputs":[{"name":"stdout","text":"kaggle.json has been successfully moved and configured.\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"!kaggle kernels output subram/formality-style-transfer-finetuned -p /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T05:55:50.352101Z","iopub.execute_input":"2024-11-20T05:55:50.352775Z","iopub.status.idle":"2024-11-20T05:55:52.164496Z","shell.execute_reply.started":"2024-11-20T05:55:50.352700Z","shell.execute_reply":"2024-11-20T05:55:52.163344Z"}},"outputs":[{"name":"stdout","text":"Kernel log downloaded to /kaggle/working/formality-style-transfer-finetuned.log \n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}