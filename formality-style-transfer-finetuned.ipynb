{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9935893,"sourceType":"datasetVersion","datasetId":6108255}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/subram/formality-style-transfer-finetuned?scriptVersionId=208471382\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"file_path = '/kaggle/input/gyafcdatae/Entertainment_Music/train/informal'   # Replace 'your_text_file.txt' with the path to your text file\nwith open(file_path, 'r') as file:\n    X_train = file.readlines()\n\nfor i  in range(len(X_train)):\n  X_train[i] = X_train[i].split(\"\\n\")[0]\n  X_train[i] = X_train[i].lower()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(X_train))\nX_train[30000:30010]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read text from a file\nfile_path = '/kaggle/input/gyafcdatae/Entertainment_Music/train/formal'\nwith open(file_path, 'r') as file:\n    y_train = file.readlines()\n\nfor i  in range(len(y_train)):\n  y_train[i] = y_train[i].split(\"\\n\")[0]\n  y_train[i] = y_train[i].lower()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(y_train))\ny_train[30000:30010]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read text from a file\nfile_path = '/kaggle/input/gyafcdatae/Entertainment_Music/test/informal'\nwith open(file_path, 'r') as file:\n    X_test = file.readlines()\n\nfor i  in range(len(X_test)):\n  X_test[i] = X_test[i].split(\"\\n\")[0]\n  X_test[i] = X_test[i].lower()\n\nprint(len(X_test))\nX_test[:10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read text from a file\nfile_path = '/kaggle/input/gyafcdatae/Entertainment_Music/test/formal.ref2'\nwith open(file_path, 'r') as file:\n    y_test = file.readlines()\n\nfor i  in range(len(y_test)):\n  y_test[i] = y_test[i].split(\"\\n\")[0]\n  y_test[i] = y_test[i].lower()\n\nprint(len(y_test))\ny_test[:10]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torch\n\nclass ChatData(Dataset):\n    def __init__(self, tokenizer, X_train, y_train, subset_range=(0, None), max_length=64):\n        \"\"\"\n        Initializes the dataset for fine-tuning GPT-2.\n        \n        Args:\n        - tokenizer: The tokenizer to use for encoding data.\n        - X_train: List of informal sentences.\n        - y_train: List of corresponding formal sentences.\n        - subset_range: Tuple specifying the range of data to use (start_idx, end_idx).\n        - max_length: Maximum length for tokenization (default=64).\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.X = X_train\n        self.y = y_train\n        \n        # Apply subset range if specified\n        start, end = subset_range\n        end = end if end is not None else len(self.X)\n        self.X = self.X[start:end]\n        self.y = self.y[start:end]\n        \n        # Format the data with prefixes and suffixes\n        formatted_data = []\n        for idx, (i, target) in enumerate(zip(self.X, self.y)):\n            try:\n                formatted_data.append(\n                    f\"<|startoftext|>\\n[Informal]: {i}\\n [Formal]: {target} <|endoftext|>\"\n                )\n            except Exception as e:\n                print(f\"Error processing index {idx}: {e}\")\n        \n        self.X = formatted_data\n        print(f\"Sample input after formatting: {self.X[0]}\")  # Debugging: print first example\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Tokenizes the sample on-the-fly to avoid memory issues.\n        \"\"\"\n        text = self.X[idx]\n        encoded = self.tokenizer(\n            text, \n            max_length=self.max_length, \n            truncation=True, \n            padding=\"max_length\", \n            return_tensors=\"pt\"\n        )\n        input_ids = encoded['input_ids'].squeeze(0)\n        attention_mask = encoded['attention_mask'].squeeze(0)\n        return input_ids, attention_mask\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade jupyterlab jupyterlab_widgets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nimport tqdm\nimport torch\n\n# Training function\ndef train(chatData, model, tokenizer, optim, device):\n    epochs = 20\n    for epoch in tqdm.tqdm(range(epochs), desc=\"Training\"):\n        model.train()  # Ensure model is in training mode\n        total_loss = 0\n        for X, a in chatData:\n            X = X.to(device)\n            a = a.to(device)\n            \n            optim.zero_grad()\n            outputs = model(X, attention_mask=a, labels=X)\n            loss = outputs.loss\n            loss.backward()\n            optim.step()\n            total_loss += loss.item()\n        \n        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(chatData):.4f}\")\n\n        # Save tokenizer and model configurations\n        tokenizer.save_pretrained(\"tokenizer_configs\")\n        model.save_pretrained(\"model_configs\")\n        # torch.save(model.state_dict(), f\"model_state_epoch_{epoch + 1}.pt\")\n        torch.save(model.state_dict(), \"model_state.pt\")\n        \n        # Run inference for debugging\n        print(\"Example output:\", infer('Damn,u look fine!', tokenizer, model, device))\n\n# Inference function\ndef infer(inp, tokenizer, model, device):\n    inp = f\"<|startoftext|>\\n[Informal]: {inp}\\n [Formal]: \"\n    encoded = tokenizer(\n        inp, \n        return_tensors=\"pt\", \n        max_length=30, \n        truncation=True, \n        padding=\"max_length\"\n    )\n    X = encoded[\"input_ids\"].to(device)\n    a = encoded[\"attention_mask\"].to(device)\n    \n    output = model.generate(\n        X, attention_mask=a, max_new_tokens=10)\n    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return output_text\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Tokenizer setup\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\ntokenizer.add_special_tokens({\n    \"pad_token\": \"<pad>\",\n    \"bos_token\": \"<|startoftext|>\",\n    \"eos_token\": \"<|endoftext|>\"\n})\ntokenizer.add_tokens([\"[Informal]:\", \"[Formal]:\"])\n\n# Model setup\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\nmodel.resize_token_embeddings(len(tokenizer))  # Adjust for new tokens\nmodel = model.to(device)\n\n# Dataset and DataLoader\nchatData = ChatData(tokenizer, X_train, y_train, subset_range=(0, len(X_train)), max_length=30)\nchatDataLoader = DataLoader(chatData, batch_size=16, shuffle=True)\n\n# Optimizer\noptim = Adam(model.parameters(), lr=1e-4)\n\n# Training\nprint(\"Starting training...\")\ntrain(chatDataLoader, model, tokenizer, optim, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\n# Ensure the destination directory exists\nkaggle_config_dir = os.path.expanduser('~/.config/kaggle/')\nos.makedirs(kaggle_config_dir, exist_ok=True)\n\n# Move the kaggle.json file\nshutil.copy('/kaggle/input/kaggles/kaggle.json', os.path.join(kaggle_config_dir, 'kaggle.json'))\n\nprint(\"kaggle.json file moved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T19:16:06.162781Z","iopub.execute_input":"2024-11-19T19:16:06.163204Z","iopub.status.idle":"2024-11-19T19:16:06.174168Z","shell.execute_reply.started":"2024-11-19T19:16:06.16317Z","shell.execute_reply":"2024-11-19T19:16:06.173036Z"}},"outputs":[{"name":"stdout","text":"kaggle.json file moved successfully!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!kaggle kernels output subram/notebook6583ea48bc -p /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T19:16:09.884326Z","iopub.execute_input":"2024-11-19T19:16:09.884703Z","iopub.status.idle":"2024-11-19T19:16:53.192419Z","shell.execute_reply.started":"2024-11-19T19:16:09.884669Z","shell.execute_reply":"2024-11-19T19:16:53.191338Z"}},"outputs":[{"name":"stdout","text":"Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.config/kaggle/kaggle.json'\nOutput file downloaded to /kaggle/working/model_configs/config.json\nOutput file downloaded to /kaggle/working/model_configs/generation_config.json\nOutput file downloaded to /kaggle/working/model_configs/model.safetensors\nOutput file downloaded to /kaggle/working/model_state.pt\nOutput file downloaded to /kaggle/working/tokenizer_configs/added_tokens.json\nOutput file downloaded to /kaggle/working/tokenizer_configs/merges.txt\nOutput file downloaded to /kaggle/working/tokenizer_configs/special_tokens_map.json\nOutput file downloaded to /kaggle/working/tokenizer_configs/tokenizer_config.json\nOutput file downloaded to /kaggle/working/tokenizer_configs/vocab.json\nKernel log downloaded to /kaggle/working/notebook6583ea48bc.log \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"/kaggle/working/tokenizer_configs\")\nmodel = GPT2LMHeadModel.from_pretrained(\"/kaggle/working/model_configs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T19:18:34.975225Z","iopub.execute_input":"2024-11-19T19:18:34.976066Z","iopub.status.idle":"2024-11-19T19:18:40.421695Z","shell.execute_reply.started":"2024-11-19T19:18:34.976026Z","shell.execute_reply":"2024-11-19T19:18:40.420632Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install sentence_transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T19:18:45.218326Z","iopub.execute_input":"2024-11-19T19:18:45.218813Z","iopub.status.idle":"2024-11-19T19:18:55.761913Z","shell.execute_reply.started":"2024-11-19T19:18:45.21878Z","shell.execute_reply":"2024-11-19T19:18:55.760928Z"}},"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-3.3.1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\n\ndef infer(inp, tokenizer, model, device):\n    inp = f\"<|startoftext|>\\n[Informal]: {inp}\\n [Formal]: \"\n    encoded = tokenizer(\n        inp, \n        return_tensors=\"pt\", \n        max_length=30, \n        truncation=True, \n        padding=\"max_length\"\n    )\n    X = encoded[\"input_ids\"].to(device)\n    a = encoded[\"attention_mask\"].to(device)\n    \n    output = model.generate(\n        X, attention_mask=a, max_new_tokens=10)\n    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return output_text\n\n# Ensure proper device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)  # Move model to the correct device\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T19:19:08.393623Z","iopub.execute_input":"2024-11-19T19:19:08.394004Z","iopub.status.idle":"2024-11-19T19:19:09.109603Z","shell.execute_reply.started":"2024-11-19T19:19:08.393969Z","shell.execute_reply":"2024-11-19T19:19:09.108475Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"inp = input()\nprint(infer(inp, tokenizer, model, device))\n# print(infer(inp))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T19:22:22.854566Z","iopub.execute_input":"2024-11-19T19:22:22.855376Z","iopub.status.idle":"2024-11-19T19:22:28.521219Z","shell.execute_reply.started":"2024-11-19T19:22:22.855339Z","shell.execute_reply":"2024-11-19T19:22:28.520243Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":" U are rude dude\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n [Informal]:  U are rude dude\n  [Formal]:   you are rude, my friend. \n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}